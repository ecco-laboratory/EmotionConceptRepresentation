{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import pearsonr, norm\n",
    "from pingouin import partial_corr\n",
    "import matplotlib.lines as mlines\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_z(r):\n",
    "    return 0.5 * np.log((1 + r) / (1 - r))\n",
    "def reverse_fisher_z(z):\n",
    "    return (np.exp(2*z) - 1) / (np.exp(2*z) + 1)\n",
    "\n",
    "def load_ratings_data_with_movies(va_dir, category_dir, subject, brain_region, data_type, is_yhat=True):\n",
    "    if data_type == 'valence_arousal' or data_type == 'binary_valence_arousal':\n",
    "        dir_path = va_dir\n",
    "    elif data_type == 'category':\n",
    "        dir_path = category_dir\n",
    "    else:\n",
    "        raise ValueError(\"Invalid data type. Choose 'valence_arousal' or 'category'.\")\n",
    "    \n",
    "    file_prefix = 'yhat' if is_yhat else 'y'\n",
    "    file_path = os.path.join(dir_path, f'{file_prefix}_sub-{subject}_{brain_region}.csv')\n",
    "    if not os.path.exists(file_path) and brain_region == 'vmPFC_a24_included':\n",
    "        file_path = os.path.join(dir_path, f'{file_prefix}_sub-{subject}_vmPFC.csv')\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "def load_TEM_data_with_movies(TEM_dir, subject, brain_region, component, freq_name, is_yhat=True):\n",
    "    file_prefix = 'yhat' if is_yhat else 'y'\n",
    "    file_path = os.path.join(TEM_dir, f'{file_prefix}_sub-{subject}_{brain_region}_{component}_{freq_name}.csv')\n",
    "    if not os.path.exists(file_path) and brain_region == 'vmPFC_a24_included':\n",
    "        file_path = os.path.join(TEM_dir, f'{file_prefix}_sub-{subject}_vmPFC_{component}_{freq_name}.csv')\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    return data\n",
    "\n",
    "def cross_decoding_TEMRatings(data, emo_labels, if_output_cross_decoding_yhat=False):\n",
    "\n",
    "    yhat_TEM, y_rating = data\n",
    "    pred_outcome_corr = {}\n",
    "    cross_decoding_yhat = pd.DataFrame()\n",
    "\n",
    "    X = yhat_TEM.copy()\n",
    "    X = X.drop(columns=['movie']).values\n",
    "    y = y_rating[emo_labels].values  \n",
    "    target_labels = emo_labels\n",
    "\n",
    "    #make sure order of movies movie column is matched\n",
    "    assert (yhat_TEM['movie'] == y_rating['movie']).all(), 'Movie order is not matched'\n",
    "        \n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    # Perform cross-validation by movie group\n",
    "    for train_idx, test_idx in logo.split(X, y, groups=y_rating['movie']):\n",
    "        # Perform PLS regression\n",
    "        model = PLSRegression(n_components=20)\n",
    "        model.fit(X[train_idx], y[train_idx])\n",
    "        predictions = model.predict(X[test_idx])\n",
    "        predictions_wlabels = {}\n",
    "        # Calculate correlation for each target label \n",
    "        for label in target_labels:\n",
    "            corr = np.corrcoef(predictions[:, target_labels.index(label)], y[test_idx][:, target_labels.index(label)])[0, 1]\n",
    "            if label not in pred_outcome_corr:\n",
    "                pred_outcome_corr[label] = []\n",
    "            pred_outcome_corr[label].append(corr)\n",
    "            if if_output_cross_decoding_yhat:\n",
    "                predictions_wlabels[label] = predictions[:, target_labels.index(label)]\n",
    "        predictions_wlabels=pd.DataFrame(predictions_wlabels)\n",
    "        #add current group/movie names to the predictions_wlabels df\n",
    "        predictions_wlabels['movie'] = y_rating['movie'].values[test_idx]\n",
    "        cross_decoding_yhat = pd.concat([cross_decoding_yhat, predictions_wlabels], ignore_index=True)\n",
    "    \n",
    "    # Calculate the mean correlation for each target label across cross-validation iterations (movies)\n",
    "    mean_corr = {label: np.mean(pred_outcome_corr[label]) for label in target_labels}\n",
    "    if if_output_cross_decoding_yhat:\n",
    "        return mean_corr, cross_decoding_yhat\n",
    "    else:\n",
    "        return mean_corr\n",
    "\n",
    "def mediation_analysis(yhat_BrainToRatings, yhat_BrainToTEM, y_ratings, labels, n_bootstraps=1000):\n",
    "    results = []\n",
    "\n",
    "    for label in labels:\n",
    "        X = sm.add_constant(yhat_BrainToRatings[label])  # Add intercept\n",
    "\n",
    "        #yhat_BrainToTEM ~ yhat_BrainToRatings\n",
    "        mediator_model = sm.OLS(yhat_BrainToTEM[label], X).fit()\n",
    "        a = mediator_model.params[1]  # Effect of predictor on mediator\n",
    "\n",
    "        # y_ratings ~ yhat_BrainToRatings + yhat_BrainToTEM\n",
    "        X2 = sm.add_constant(pd.concat([yhat_BrainToRatings[label], yhat_BrainToTEM[label]], axis=1))\n",
    "        outcome_model = sm.OLS(y_ratings[label], X2).fit()\n",
    "        b = outcome_model.params[2]  # effect of mediator on outcome (controlling for predictor)\n",
    "        c_prime = outcome_model.params[1]  # direct effect of predictor on outcome\n",
    "\n",
    "        indirect_effect = a * b\n",
    "        total_effect = indirect_effect + c_prime  \n",
    "\n",
    "        # Bootstrapping for significance testing\n",
    "        boot_indirect = []\n",
    "        for _ in range(n_bootstraps):\n",
    "            idx = resample(range(len(yhat_BrainToRatings)), replace=True)\n",
    "            X_boot = sm.add_constant(yhat_BrainToRatings.iloc[idx][label])\n",
    "            mediator_model_boot = sm.OLS(yhat_BrainToTEM.iloc[idx][label], X_boot).fit()\n",
    "            a_boot = mediator_model_boot.params[1]\n",
    "\n",
    "            X2_boot = sm.add_constant(pd.concat([yhat_BrainToRatings.iloc[idx][label], yhat_BrainToTEM.iloc[idx][label]], axis=1))\n",
    "            outcome_model_boot = sm.OLS(y_ratings.iloc[idx][label], X2_boot).fit()\n",
    "            b_boot = outcome_model_boot.params[2]\n",
    "\n",
    "            boot_indirect.append(a_boot * b_boot)\n",
    "\n",
    "        boot_indirect = np.array(boot_indirect)\n",
    "        p_value = (np.sum(boot_indirect <= 0) + np.sum(boot_indirect >= 0)) / n_bootstraps  # Two-tailed test\n",
    "\n",
    "        results.append({\n",
    "            \"Category\": label,\n",
    "            \"a (Predictor → Mediator)\": a,\n",
    "            \"b (Mediator → Outcome)\": b,\n",
    "            \"c' (Direct Effect)\": c_prime,\n",
    "            \"Indirect Effect (a*b)\": indirect_effect,\n",
    "            \"Total Effect\": total_effect,\n",
    "            \"p-value (bootstrapped)\": p_value\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def fisher_r_to_z(r):\n",
    "    return 0.5 * np.log((1 + r) / (1 - r))\n",
    "\n",
    "def get_partial_corrs(yhat_BrainToRatings, yhat_BrainToTEM, y_ratings, labels):\n",
    "    results = []\n",
    "\n",
    "    for label in labels:\n",
    "        results_r_xy_movie = []\n",
    "        results_r_xy_given_m_movie = []\n",
    "        for movie in yhat_BrainToRatings['movie'].unique():\n",
    "            yhat_BrainToRatings_movie = yhat_BrainToRatings[yhat_BrainToRatings['movie'] == movie][label]\n",
    "            yhat_BrainToTEM_movie = yhat_BrainToTEM[yhat_BrainToTEM['movie'] == movie][label]\n",
    "            y_ratings_movie = y_ratings[y_ratings['movie'] == movie][label]\n",
    "            # Calculate Pearson correlation\n",
    "            r_xy, p_xy = pearsonr(yhat_BrainToRatings_movie, y_ratings_movie)\n",
    "            # Partial correlation controlling for mediator\n",
    "            df = pd.DataFrame({\n",
    "                \"X\": yhat_BrainToRatings_movie, \n",
    "                \"M\": yhat_BrainToTEM_movie, \n",
    "                \"Y\": y_ratings_movie\n",
    "            })\n",
    "            partial_corr_result = partial_corr(data=df, x=\"X\", y=\"Y\", covar=\"M\", method=\"pearson\")\n",
    "            r_xy_given_m = partial_corr_result[\"r\"].values[0]  # Partial correlation\n",
    "            results_r_xy_movie.append(r_xy)\n",
    "            results_r_xy_given_m_movie.append(r_xy_given_m)\n",
    "        #average across movies\n",
    "        r_xy = np.mean(results_r_xy_movie)\n",
    "        r_xy_given_m = np.mean(results_r_xy_given_m_movie)\n",
    "        # Convert to Fisher's z\n",
    "        r_xy_z = fisher_r_to_z(r_xy)\n",
    "        r_xy_given_m_z = fisher_r_to_z(r_xy_given_m)\n",
    "  \n",
    "        results.append({\n",
    "            \"Emotion\": label,\n",
    "            \"r_xy\": r_xy,\n",
    "            \"r_xy_given_m\": r_xy_given_m,\n",
    "            \"r_xy_z\": r_xy_z,\n",
    "            \"r_xy_given_m_z\": r_xy_given_m_z\n",
    "        })\n",
    "    return pd.DataFrame(results)          \n",
    "\n",
    "\n",
    "def partial_correlation_BrainTEMRatings(TEM_dir, va_dir, category_dir, subjects, categories, va_ratings, emo_types=['category', 'valence_arousal','valence_arousal'], brain_regions=['Hippocampus', 'EntorhinalCortex','vmPFC_a24_included'], components=['p','g','g'], freq_names=['freq01234','freq01234','freq01234']):\n",
    "    for subject in tqdm(subjects):\n",
    "        for emo_type, brain_region, component, freq_name in zip(emo_types, brain_regions, components, freq_names):\n",
    "            labels = categories if emo_type == 'category' else va_ratings\n",
    "            y_ratings = load_ratings_data_with_movies(va_dir, category_dir, subject, brain_region, emo_type, is_yhat=False)\n",
    "            yhat_BrainToRatings = load_ratings_data_with_movies(va_dir, category_dir, subject, brain_region, emo_type, is_yhat=True)\n",
    "            yhat_BrainToTEM = load_TEM_data_with_movies(TEM_dir, subject, brain_region, component, freq_name, is_yhat=True)\n",
    "            # Perform cross-decoding\n",
    "            _, yhat_BrainToTEMToRating = cross_decoding_TEMRatings([yhat_BrainToTEM, y_ratings], labels, if_output_cross_decoding_yhat=True)\n",
    "            #mediation_results = mediation_analysis(yhat_BrainToRatings, yhat_BrainToTEMToRating, y_ratings, labels)\n",
    "            partial_corr_results = get_partial_corrs(yhat_BrainToRatings, yhat_BrainToTEMToRating, y_ratings, labels)\n",
    "            partial_corr_results['subject'] = subject\n",
    "            partial_corr_results['region'] = brain_region\n",
    "            partial_corr_results['TEM_component'] = component\n",
    "            partial_corr_results['emo_type'] = emo_type\n",
    "            partial_corr_results['freq'] = freq_name\n",
    "            if subject == subjects[0]:\n",
    "                partial_corr_df = partial_corr_results\n",
    "            else:\n",
    "                partial_corr_df = pd.concat([partial_corr_df, partial_corr_results], ignore_index=True)\n",
    "    return partial_corr_df\n",
    "\n",
    "\n",
    "def get_TEMRating_cross_decoding_df(TEM_dir, va_dir, category_dir, ori_va_dir, ori_category_dir, subjects, brain_regions, categories, va_ratings, TEM_components):\n",
    "    pred_outcome_corr_TEMcategory = []\n",
    "    pred_outcome_corr_TEMva = []\n",
    "\n",
    "    for emo_type, labels in zip(['category', 'valence_arousal'], [categories, va_ratings]):\n",
    "        subject_column, region_column, component_column = [], [], []\n",
    "        for subject in tqdm(subjects):\n",
    "            for brain_region in brain_regions:\n",
    "                for component in TEM_components:\n",
    "                    y_rating = load_ratings_data_with_movies(va_dir, category_dir, subject, brain_region, emo_type, is_yhat=False)\n",
    "                    yhat_TEM = load_TEM_data_with_movies(TEM_dir, subject, brain_region, component, 'freq01234', is_yhat=True)\n",
    "                    # Perform cross-decoding \n",
    "                    if emo_type == 'category':\n",
    "                        pred_outcome_corr_TEMcategory.append(cross_decoding_TEMRatings([yhat_TEM, y_rating], labels))\n",
    "                    else:\n",
    "                        pred_outcome_corr_TEMva.append(cross_decoding_TEMRatings([yhat_TEM, y_rating], labels))\n",
    "                    subject_column.append(subject)\n",
    "                    region_column.append(brain_region)\n",
    "                    component_column.append(component)\n",
    "        if emo_type == 'category':\n",
    "            pred_outcome_corr_TEMcategory = pd.DataFrame(pred_outcome_corr_TEMcategory)\n",
    "            pred_outcome_corr_TEMcategory['subject'] = subject_column\n",
    "            pred_outcome_corr_TEMcategory['region'] = region_column\n",
    "            pred_outcome_corr_TEMcategory['TEMcomponent'] = component_column\n",
    "        else:\n",
    "            pred_outcome_corr_TEMva = pd.DataFrame(pred_outcome_corr_TEMva)\n",
    "            pred_outcome_corr_TEMva['subject'] = subject_column\n",
    "            pred_outcome_corr_TEMva['region'] = region_column\n",
    "            pred_outcome_corr_TEMva['TEMcomponent'] = component_column\n",
    "\n",
    "    pred_outcome_corr_TEMva['target'] = 'valence_arousal'\n",
    "    pred_outcome_corr_TEMcategory['target'] = 'category'\n",
    "    #melt each and combine\n",
    "    pred_outcome_corr_TEMva = pd.melt(pred_outcome_corr_TEMva, id_vars=['subject', 'region', 'target', 'TEMcomponent'], var_name='label', value_name='corr')\n",
    "    pred_outcome_corr_TEMva['corr_z'] = fisher_z(pred_outcome_corr_TEMva['corr'])\n",
    "    pred_outcome_corr_TEMcategory = pd.melt(pred_outcome_corr_TEMcategory, id_vars=['subject', 'region', 'target', 'TEMcomponent'], var_name='label', value_name='corr')\n",
    "    pred_outcome_corr_TEMcategory['corr_z'] = fisher_z(pred_outcome_corr_TEMcategory['corr'])\n",
    "    pred_outcome_corr = pd.concat([pred_outcome_corr_TEMva, pred_outcome_corr_TEMcategory], ignore_index=True)\n",
    "\n",
    "    ori_pred_outcome_corr_category = pd.read_csv(ori_category_dir)\n",
    "    ori_pred_outcome_corr_va = pd.read_csv(ori_va_dir)\n",
    "    ori_pred_outcome_corr_category = ori_pred_outcome_corr_category[ori_pred_outcome_corr_category['region'].isin(brain_regions)]\n",
    "    ori_pred_outcome_corr_va = ori_pred_outcome_corr_va[ori_pred_outcome_corr_va['region'].isin(brain_regions)]\n",
    "    #melt the original data\n",
    "    ori_pred_outcome_corr_category = pd.melt(ori_pred_outcome_corr_category, id_vars=['subject', 'region'], var_name='label', value_name='corr')\n",
    "    ori_pred_outcome_corr_category['target'] = 'category'\n",
    "    ori_pred_outcome_corr_category['TEMcomponent'] = np.nan\n",
    "    ori_pred_outcome_corr_va = pd.melt(ori_pred_outcome_corr_va, id_vars=['subject', 'region'], var_name='label', value_name='corr')\n",
    "    ori_pred_outcome_corr_va['target'] = 'valence_arousal'\n",
    "    ori_pred_outcome_corr_va['TEMcomponent'] = np.nan\n",
    "    #combine the original data\n",
    "    ori_pred_outcome_corr = pd.concat([ori_pred_outcome_corr_category, ori_pred_outcome_corr_va], ignore_index=True)\n",
    "    ori_pred_outcome_corr['decoding_type'] = 'within'\n",
    "    ori_pred_outcome_corr['corr_z'] = fisher_z(ori_pred_outcome_corr['corr'])\n",
    "    #combine the original and new data\n",
    "    pred_outcome_corr['decoding_type'] = 'cross'\n",
    "    #match the subject format\n",
    "    pred_outcome_corr['subject'] = ['sub-'+ subj for subj in pred_outcome_corr['subject']]\n",
    "    all_pred_outcome_corr = pd.concat([ori_pred_outcome_corr, pred_outcome_corr], ignore_index=True)\n",
    "    return all_pred_outcome_corr\n",
    "\n",
    "def plot_within_corss_comparison(all_pred_outcome_corr, brain_regions, components):\n",
    "\n",
    "    plt.figure(figsize=(8*len(brain_regions), 8*len(components)))\n",
    "\n",
    "    for i, region in enumerate(brain_regions):\n",
    "        for c, component in enumerate(components):\n",
    "          \n",
    "            all_pred_outcome_corr_subset = all_pred_outcome_corr[(all_pred_outcome_corr['region'] == region) & ((all_pred_outcome_corr['TEMcomponent'] == component) | (all_pred_outcome_corr['TEMcomponent'].isna()))]\n",
    "            all_pred_outcome_corr_subset = all_pred_outcome_corr_subset.drop(columns=['TEMcomponent'])\n",
    "            plt.subplot(len(components), len(brain_regions), i*len(components)+c+1)\n",
    "            sns.stripplot(x='label', y='corr', hue='decoding_type', data=all_pred_outcome_corr_subset, jitter=True, dodge=True, alpha=0.7, palette='Set2')\n",
    "            plt.title(f'TEM {component} yhat to Ratings Cross-Decoding Performance\\nin {region}')\n",
    "            plt.xlabel('')\n",
    "            plt.ylabel('Prediction-Outcome Correlation')\n",
    "            if i > 0:\n",
    "                plt.ylabel('')\n",
    "            plt.legend(title='Decoding Type', loc='upper right')\n",
    "            plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_corss_decoding_performance(all_pred_outcome_corr, brain_regions):\n",
    "\n",
    "    plt.figure(figsize=(8*len(brain_regions), 8))\n",
    "\n",
    "    all_pred_outcome_corr = all_pred_outcome_corr[all_pred_outcome_corr['TEMcomponent'].notna()]\n",
    "    for i, region in enumerate(brain_regions):\n",
    "        all_pred_outcome_corr_subset = all_pred_outcome_corr[all_pred_outcome_corr['region'] == region]\n",
    "        plt.subplot(1, len(brain_regions), i+1)\n",
    "        sns.stripplot(x='label', y='corr', hue='TEMcomponent', data=all_pred_outcome_corr_subset, jitter=True, dodge=True, alpha=0.7, palette='Set2')\n",
    "        plt.title(f'TEM yhat to Ratings Cross-Decoding Performance\\nin {region}')\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('Prediction-Outcome Correlation')\n",
    "        if i > 0:\n",
    "            plt.ylabel('')\n",
    "        plt.legend(title='TEM Component', loc='upper right')\n",
    "        plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_subjectConnected_performance_scatter(performance_all, title, legend_title, outer_x_order, outer_x_labels, \n",
    "                                              outer_x_column_name, inner_x_order, inner_x_labels, inner_x_column_name, \n",
    "                                              y_column_name, inner_x_colors, subject_column_name, outer_shift=1.5, \n",
    "                                              inner_shift=0.8, jitter=0.2, figsize=(12, 6), legend_loc='upper left', \n",
    "                                              legend_bbox_to_anchor=(1, 1), yrange=None, yaxis_label='Prediction-Outcome Correlation', \n",
    "                                              xlabel='', save=None, ax=None, xtick_rotation=None):\n",
    "    \n",
    "    inner_x_shift = {group: idx * inner_shift * outer_shift for idx, group in enumerate(inner_x_order)}\n",
    "    outer_x_shift = {group: idx * len(inner_x_order) * outer_shift for idx, group in enumerate(outer_x_order)}\n",
    "    \n",
    "    performance_all['outer_shifted'] = performance_all.apply(\n",
    "        lambda row: outer_x_shift[row[outer_x_column_name]] + \n",
    "                    inner_x_shift[row[inner_x_column_name]] + \n",
    "                    np.random.uniform(-jitter, jitter), axis=1\n",
    "    )\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial']\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    scatter = sns.scatterplot(data=performance_all, x='outer_shifted', y=y_column_name, hue=inner_x_column_name, \n",
    "                              s=100, alpha=0.5, palette=inner_x_colors, ax=ax)\n",
    "\n",
    "    # subject-wise connection lines\n",
    "    subjects = performance_all[subject_column_name].unique()\n",
    "    for subject in subjects:\n",
    "        subject_data = performance_all[performance_all[subject_column_name] == subject]\n",
    "        for outer_group in outer_x_order:\n",
    "            inner_data = subject_data[subject_data[outer_x_column_name] == outer_group]\n",
    "            inner_data = inner_data.sort_values('outer_shifted')\n",
    "            if len(inner_data) > 1:\n",
    "                ax.plot(inner_data['outer_shifted'], inner_data[y_column_name], \n",
    "                        color='gray', alpha=0.5, lw=1)\n",
    "    \n",
    "    # mean lines for each inner-outer combination\n",
    "    mean_line_handles = []\n",
    "    for outer_group in outer_x_order:\n",
    "        for inner_group in inner_x_order:\n",
    "            subset = performance_all[(performance_all[outer_x_column_name] == outer_group) & (performance_all[inner_x_column_name] == inner_group)]\n",
    "            if not subset.empty:\n",
    "                mean_y = subset[y_column_name].mean()\n",
    "                mean_x = outer_x_shift[outer_group] + inner_x_shift[inner_group]\n",
    "                ax.plot([mean_x - jitter, mean_x + jitter], [mean_y, mean_y], \n",
    "                        color='black', lw=2, label='_mean_line_')  # Avoid duplicate labels\n",
    "                mean_line_handles.append(mlines.Line2D([], [], color='black', lw=2, label=\"Mean\"))\n",
    "\n",
    "    xticks, xticklabels = [], []\n",
    "    for outer_idx, outer_group in enumerate(outer_x_order):\n",
    "        xticks.append(outer_x_shift[outer_group] + np.mean([inner_x_shift[i] for i in inner_x_order]))\n",
    "        xticklabels.append(outer_x_labels[outer_idx])\n",
    "    \n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(xticklabels, weight='bold')\n",
    "    ax.set_xlabel(xlabel, weight='bold')\n",
    "    ax.set_ylabel(yaxis_label, weight='bold')\n",
    "    ax.set_title(title, weight='bold')\n",
    "\n",
    "    if yrange:\n",
    "        ax.set_ylim(yrange)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.grid(False)\n",
    "\n",
    "    legend_handles = [mlines.Line2D([], [], marker='o', color=color, linestyle='None', markersize=10, label=label, alpha=0.5) \n",
    "                      for label, color in zip(inner_x_labels, inner_x_colors)]\n",
    "    legend_handles.append(mlines.Line2D([], [], color='black', lw=2, label=\"Mean\"))  # Mean line\n",
    "    ax.legend(handles=legend_handles, title=legend_title, loc=legend_loc, bbox_to_anchor=legend_bbox_to_anchor)\n",
    "\n",
    "    if xtick_rotation:\n",
    "        plt.xticks(rotation=xtick_rotation)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(save, dpi=600)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_dir = './outputs/rep3/ratings_prediction_yhat/brain/valence_arousal'\n",
    "category_dir = './outputs/rep3/ratings_prediction_yhat/brain//category'\n",
    "ori_category_dir = './outputs/rep3/ratings_prediction_performance/brain/category/categoryRatings_prediction_performance_generalized_across_movies_hcecvmpfc.csv'\n",
    "ori_va_dir = './outputs/rep3/ratings_prediction_performance/brain/valence_arousal/valenceArousalRatings_prediction_performance_generalized_across_movies_hcecvmpfc.csv'\n",
    "brain_regions = ['Hippocampus', 'EntorhinalCortex', 'vmPFC']#['anteriorHippocampus', 'posteriorHippocampus']\n",
    "components = ['p','g']\n",
    "categories = ['Anger', 'Anxiety', 'Fear', 'Surprise', 'Guilt', 'Disgust', 'Sad', 'Regard', \n",
    "            'Satisfaction', 'WarmHeartedness', 'Happiness', 'Pride', 'Love']\n",
    "va_ratings = ['Good', 'Bad', 'Calm', 'AtEase']\n",
    "TEM_dir = './outputs/rep3/ratings_prediction_yhat/brainToTEM/MDSseed121/iteration32000/walkRandomSeed42'\n",
    "# Get subject list\n",
    "subjects = list(set([\n",
    "fname.split('/')[-1].split('_')[1].split('-')[1]\n",
    "for fname in glob.glob(os.path.join(va_dir, 'yhat_sub-S*.csv'))\n",
    "]))\n",
    "#partial_corr_df = partial_correlation_BrainTEMRatings(TEM_dir, va_dir, category_dir, subjects, categories, va_ratings)\n",
    "partial_corr_df = partial_correlation_BrainTEMRatings(TEM_dir, va_dir, category_dir, subjects, categories, va_ratings,\n",
    "                                                      emo_types=['category', 'valence_arousal'], \n",
    "                                                      brain_regions=['Hippocampus','Hippocampus'], \n",
    "                                                      components=['p','p'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_corr_df_melted = pd.melt(partial_corr_df, id_vars=['subject', 'region', 'TEM_component', 'emo_type','Emotion','freq'], value_vars=['r_xy_z', 'r_xy_given_m_z'], var_name='type', value_name='zvalue')\n",
    "partial_corr_df_melted['corr_type'] = partial_corr_df_melted['type'].replace({'r_xy_given_m_z':'partial_corr', 'r_xy_z':'corr'})\n",
    "partial_corr_df_melted = partial_corr_df_melted.drop(columns='type')\n",
    "ori_df = pd.read_csv('./outputs/performance_partial_corr_brainToTEMtoRatings_apHC_it32000.csv')\n",
    "partial_corr_df_melted = pd.concat([ori_df, partial_corr_df_melted], ignore_index=True)\n",
    "partial_corr_df_melted.to_csv('./outputs/performance_partial_corr_brainToTEMtoRatings_it32000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean diff\n",
      "emo_type\n",
      "category          -0.004047\n",
      "valence_arousal   -0.000214\n",
      "Name: partial_corr_diff, dtype: float64\n",
      "          emo_type      Mean  CI_Lower  CI_Upper\n",
      "0         category -0.004047 -0.006273 -0.001961\n",
      "1  valence_arousal -0.000214 -0.001421  0.001004\n"
     ]
    }
   ],
   "source": [
    "partial_corr_df = pd.read_csv('./outputs/performance_partial_corr_brainToTEMtoRatings_it32000.csv')\n",
    "partial_corr_df = partial_corr_df[partial_corr_df['TEM_component'] == 'p']\n",
    "partial_corr_df['corr'] = reverse_fisher_z(partial_corr_df['zvalue'])\n",
    "partial_corr_df = partial_corr_df[partial_corr_df['region'] == 'Hippocampus']\n",
    "\n",
    "partial_corr_df = partial_corr_df.pivot_table(index=['subject', 'region', 'TEM_component', 'emo_type','Emotion'], columns='corr_type', values='corr').reset_index()\n",
    "#get partial_corr - corr difference for each row\n",
    "partial_corr_df['partial_corr_diff'] = partial_corr_df['partial_corr'] - partial_corr_df['corr']\n",
    "#get average across partial_corr_diff across Emotion and subject to get one single value for each emo_type\n",
    "print('mean diff')\n",
    "print(partial_corr_df.groupby(['emo_type'])['partial_corr_diff'].mean())\n",
    "\n",
    "\n",
    "def bootstrap_ci(data, statistic=np.mean, n_bootstrap=1000, ci=0.95):\n",
    "    data = np.asarray(data)\n",
    "\n",
    "    bootstrap_samples = np.random.choice(data, (n_bootstrap, len(data)), replace=True)\n",
    "    bootstrap_statistics = np.apply_along_axis(statistic, 1, bootstrap_samples)\n",
    "    lower_percentile = (1 - ci) / 2\n",
    "    upper_percentile = 1 - lower_percentile\n",
    "    \n",
    "    ci_lower = np.percentile(bootstrap_statistics, lower_percentile * 100)\n",
    "    ci_upper = np.percentile(bootstrap_statistics, upper_percentile * 100)\n",
    "    \n",
    "    return (ci_lower, ci_upper)\n",
    "\n",
    "def bootstrap_analysis(grouped_data):\n",
    "    results = []\n",
    "    \n",
    "    for emo_type in ['category','valence_arousal']:\n",
    "        group = grouped_data[emo_type]\n",
    "\n",
    "        ci = bootstrap_ci(group)\n",
    "        \n",
    "        results.append({\n",
    "            'emo_type': emo_type,\n",
    "            'Mean': group.mean(),\n",
    "            'CI_Lower': ci[0],\n",
    "            'CI_Upper': ci[1]\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "results = bootstrap_analysis(partial_corr_df.groupby(['emo_type','subject'])['partial_corr_diff'].mean())\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
